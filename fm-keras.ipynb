{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"ml-1m/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1::1193::5::978300760\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f= open(path+'ratings.dat')\n",
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\n",
    "ratings = np.loadtxt(path+'ratings.dat', dtype=int, delimiter='::')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1m ratings\n",
    "\n",
    "All ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
    "\n",
    "    userId,movieId,rating,timestamp\n",
    "\n",
    "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "\n",
    "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,      1193,         5, 978300760],\n",
       "       [        1,       661,         3, 978302109],\n",
       "       [        1,       914,         3, 978301968],\n",
       "       [        1,      3408,         4, 978300275],\n",
       "       [        1,      2355,         5, 978824291],\n",
       "       [        1,      1197,         3, 978302268],\n",
       "       [        1,      1287,         5, 978302039],\n",
       "       [        1,      2804,         5, 978300719],\n",
       "       [        1,       594,         4, 978302268],\n",
       "       [        1,       919,         4, 978301368]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed = 50\n",
    "\n",
    "msk = np.random.rand(len(ratings)) < 0.8\n",
    "trn = ratings[msk]\n",
    "val = ratings[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6041"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.unique(ratings[:,0]).max() + 1\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3953"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_movies = np.unique(ratings[:,1]).max() + 1\n",
    "n_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "State-of-the-art: http://www.mymedialite.net/examples/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Dot, Add, Input, Flatten, Concatenate, BatchNormalization, Activation, Lambda\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF model\n",
    "\n",
    "input\n",
    "$$(u,m)$$\n",
    "embedding\n",
    "$$\\to (e_u,e_m)$$\n",
    "dot product\n",
    "$$\\to  e_u\\cdot e_m$$\n",
    "to fit the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_factors = 10\n",
    "\n",
    "u_input = Input(shape = (1,))\n",
    "u_emb = Embedding(input_dim=n_users, output_dim=n_factors)(u_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_input = Input(shape = (1,))\n",
    "m_emb = Embedding(input_dim=n_movies, output_dim=n_factors)(m_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dot = Dot(axes=2)([u_emb,m_emb])\n",
    "out = Flatten()(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mf_model = Model(inputs=[u_input,m_input], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://keras.io/optimizers/#adam\n",
    "# learning rate = 0.001\n",
    "\n",
    "mf_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.27939034],\n",
       "       [ 3.45203161],\n",
       "       [ 4.18549109],\n",
       "       [ 4.08537817],\n",
       "       [ 4.29241276],\n",
       "       [ 3.94918108],\n",
       "       [ 4.1293025 ],\n",
       "       [ 3.88715553],\n",
       "       [ 4.09248304],\n",
       "       [ 3.85607409]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.predict([trn[:10,0], trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "799442/799442 [==============================] - 30s 37us/step - loss: 4.1347\n",
      "Epoch 2/6\n",
      "799442/799442 [==============================] - 29s 36us/step - loss: 0.9012\n",
      "Epoch 3/6\n",
      "799442/799442 [==============================] - 37s 46us/step - loss: 0.8660\n",
      "Epoch 4/6\n",
      "799442/799442 [==============================] - 29s 37us/step - loss: 0.8552\n",
      "Epoch 5/6\n",
      "799442/799442 [==============================] - 32s 40us/step - loss: 0.8412\n",
      "Epoch 6/6\n",
      "799442/799442 [==============================] - 35s 44us/step - loss: 0.8174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f03cb710>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.fit([trn[:,0], trn[:,1]], trn[:,2], epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 799442 samples, validate on 200767 samples\n",
      "Epoch 1/6\n",
      "799442/799442 [==============================] - 34s 42us/step - loss: 4.2356 - val_loss: 0.9896\n",
      "Epoch 2/6\n",
      "799442/799442 [==============================] - 34s 42us/step - loss: 0.9014 - val_loss: 0.8775\n",
      "Epoch 3/6\n",
      "799442/799442 [==============================] - 34s 42us/step - loss: 0.8639 - val_loss: 0.8668\n",
      "Epoch 4/6\n",
      "799442/799442 [==============================] - 35s 43us/step - loss: 0.8493 - val_loss: 0.8510\n",
      "Epoch 5/6\n",
      "799442/799442 [==============================] - 35s 43us/step - loss: 0.8251 - val_loss: 0.8290\n",
      "Epoch 6/6\n",
      "799442/799442 [==============================] - 33s 41us/step - loss: 0.7976 - val_loss: 0.8107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f251f160>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.fit([trn[:,0], trn[:,1]], trn[:,2], \n",
    "             validation_data = ([val[:,0],val[:,1]],val[:,2]),\n",
    "             epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF model with bias\n",
    "\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to e_u\\cdot e_m+b_u+b_m$$\n",
    "where\n",
    "$$e_u,e_m\\in R^{n_{factor}},b_u,b_m\\in R$$\n",
    "\n",
    "MF + bias + sigmoid output\n",
    "\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to 5\\sigma(e_u\\cdot e_m+b_u+b_m)\\in [0,5]$$\n",
    "\n",
    "##### Unlike other MF models, I don't treat non-existing ratings as zeros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mfb_model(n_factors = 10):\n",
    "    \n",
    "    reg = 1e-3\n",
    "    \n",
    "    u_input = Input(shape = (1,))\n",
    "    u_emb = Embedding(input_dim=n_users, \n",
    "                      output_dim=n_factors, \n",
    "                      input_length=1, \n",
    "                      embeddings_regularizer=regularizers.l2(reg))(u_input)\n",
    "\n",
    "    m_input = Input(shape = (1,))\n",
    "    m_emb = Embedding(input_dim=n_movies, \n",
    "                      output_dim=n_factors, \n",
    "                      input_length=1,\n",
    "                      embeddings_regularizer=regularizers.l2(reg))(m_input)\n",
    "        \n",
    "    u_b = Embedding(input_dim=n_users, output_dim=1, input_length=1)(u_input)\n",
    "    m_b = Embedding(input_dim=n_movies, output_dim=1, input_length=1)(m_input)\n",
    "    \n",
    "    dot = Flatten()(Dot(axes=2)([u_emb,m_emb]))\n",
    "    \n",
    "    out = Flatten()(Add()([dot,u_b,m_b]))\n",
    "\n",
    "    out = Activation('sigmoid')(out)\n",
    "    out = Lambda(lambda x:5*x)(out)\n",
    "\n",
    "    model = Model(inputs=[u_input,m_input], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfb_model = get_mfb_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mfb_model.compile(optimizer='adam', loss='mse')\n",
    "mfb_model.compile(optimizer=Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.48019314],\n",
       "       [ 2.47084451],\n",
       "       [ 2.50200295],\n",
       "       [ 2.51603889],\n",
       "       [ 2.52446055],\n",
       "       [ 2.5035429 ],\n",
       "       [ 2.48992205],\n",
       "       [ 2.46621227],\n",
       "       [ 2.45354271],\n",
       "       [ 2.4552393 ]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfb_model.predict([trn[:10,0],trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800697 samples, validate on 199512 samples\n",
      "Epoch 1/6\n",
      "800697/800697 [==============================] - 186s 233us/step - loss: 1.1314 - val_loss: 0.8594\n",
      "Epoch 2/6\n",
      "800697/800697 [==============================] - 157s 196us/step - loss: 0.8323 - val_loss: 0.8292\n",
      "Epoch 3/6\n",
      "800697/800697 [==============================] - 201s 251us/step - loss: 0.8156 - val_loss: 0.8243\n",
      "Epoch 4/6\n",
      "800697/800697 [==============================] - 188s 235us/step - loss: 0.8113 - val_loss: 0.8236\n",
      "Epoch 5/6\n",
      "800697/800697 [==============================] - 181s 226us/step - loss: 0.8098 - val_loss: 0.8232\n",
      "Epoch 6/6\n",
      "800697/800697 [==============================] - 162s 203us/step - loss: 0.8092 - val_loss: 0.8232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d26f940>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfb_model.fit([trn[:,0],trn[:,1]],trn[:,2],\n",
    "             validation_data=([val[:,0],val[:,1]],val[:,2]),\n",
    "             epochs = 6, batch_size =64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: MF + bias\n",
    "\n",
    "```\n",
    "n_factor =10\n",
    "\n",
    "Train on 799442 samples, validate on 200767 samples\n",
    "Epoch 1/6\n",
    "799442/799442 [==============================] - 39s 49us/step - loss: 1.1622 - val_loss: 0.8879\n",
    "Epoch 2/6\n",
    "799442/799442 [==============================] - 38s 48us/step - loss: 0.8572 - val_loss: 0.8529\n",
    "Epoch 3/6\n",
    "799442/799442 [==============================] - 37s 46us/step - loss: 0.8264 - val_loss: 0.8249\n",
    "Epoch 4/6\n",
    "799442/799442 [==============================] - 35s 44us/step - loss: 0.7968 - val_loss: 0.8055\n",
    "Epoch 5/6\n",
    "799442/799442 [==============================] - 36s 44us/step - loss: 0.7725 - val_loss: 0.7937\n",
    "Epoch 6/6\n",
    "799442/799442 [==============================] - 35s 44us/step - loss: 0.7494 - val_loss: 0.7823\n",
    "```\n",
    "\n",
    "Model 2: MF + bias, n_factor =50, overfitting\n",
    "```\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 118s 148us/step - loss: 2.6856 - val_loss: 0.8870\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 118s 147us/step - loss: 0.8365 - val_loss: 0.8287\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 116s 145us/step - loss: 0.7556 - val_loss: 0.7949\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 117s 146us/step - loss: 0.6772 - val_loss: 0.7861\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 113s 141us/step - loss: 0.6015 - val_loss: 0.7958\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 113s 142us/step - loss: 0.5394 - val_loss: 0.8215\n",
    "```\n",
    "\n",
    "Model 3: MF + bias, n_factor =50, regularized (l2, 1e-5)\n",
    "```\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 150s 187us/step - loss: 5.6244 - val_loss: 1.6054\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 145s 182us/step - loss: 1.1624 - val_loss: 0.9754\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 139s 174us/step - loss: 0.9061 - val_loss: 0.8804\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 146s 182us/step - loss: 0.8504 - val_loss: 0.8518\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 138s 173us/step - loss: 0.8315 - val_loss: 0.8408\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 146s 182us/step - loss: 0.8237 - val_loss: 0.8361\n",
    "```\n",
    "\n",
    "##### Model 4: MF + bias + sigmoid output\n",
    "```\n",
    "n_factor = 10\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 34s 43us/step - loss: 1.0465 - val_loss: 0.8017\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 35s 44us/step - loss: 0.7645 - val_loss: 0.7705\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 39s 49us/step - loss: 0.7237 - val_loss: 0.7541\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 44s 55us/step - loss: 0.6893 - val_loss: 0.7465\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 46s 58us/step - loss: 0.6629 - val_loss: 0.7468\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 45s 56us/step - loss: 0.6448 - val_loss: 0.7490\n",
    "\n",
    "<keras.callbacks.History at 0x11b3a69e8>\n",
    "```\n",
    "##### Sigmoid output improves 4%!\n",
    "\n",
    "Model 5: MF + bias + sigmoid output, n_factor = 50, overfitting\n",
    "```\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 126s 157us/step - loss: 1.0027 - val_loss: 0.7844\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 125s 156us/step - loss: 0.6991 - val_loss: 0.7445\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 117s 146us/step - loss: 0.5832 - val_loss: 0.7610\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 114s 142us/step - loss: 0.4992 - val_loss: 0.7998\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 112s 140us/step - loss: 0.4464 - val_loss: 0.8421\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 116s 145us/step - loss: 0.4123 - val_loss: 0.8784\n",
    "\n",
    "<keras.callbacks.History at 0x12417f978>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model\n",
    "\n",
    "linear model\n",
    "$$(u,m)\\to (e_u,e_m)\\to We_u+W'e_m\\in R$$\n",
    "where the weights $W,W'$ are independent of $u$ and $m.$\n",
    "\n",
    "DNN\n",
    "$$(u,m)\\to (e_u,e_m)\\to DNN(e_u,e_m)\\in R$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_linear_model(n_factors=10):\n",
    "    in_u = Input(shape=(1,))\n",
    "    in_m = Input(shape=(1,))\n",
    "    \n",
    "    e_u = Embedding(input_dim=n_users, output_dim=n_factors)(in_u)\n",
    "    e_m = Embedding(input_dim=n_movies, output_dim=n_factors)(in_m)\n",
    "    \n",
    "    x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "    \n",
    "    x = Dense(units=1)(x)\n",
    "    \n",
    "    return Model(inputs=[in_u,in_m], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_model = get_linear_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01178968],\n",
       "       [ 0.00800661],\n",
       "       [ 0.05860475],\n",
       "       [ 0.04867204],\n",
       "       [ 0.08399892],\n",
       "       [-0.00301455],\n",
       "       [ 0.06716891],\n",
       "       [ 0.06247176],\n",
       "       [ 0.00345044],\n",
       "       [ 0.01526604]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict([trn[:10,0],trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 799629 samples, validate on 200580 samples\n",
      "Epoch 1/6\n",
      "799629/799629 [==============================] - 102s 128us/step - loss: 1.1326 - val_loss: 0.8708\n",
      "Epoch 2/6\n",
      "799629/799629 [==============================] - 101s 126us/step - loss: 0.8484 - val_loss: 0.8476\n",
      "Epoch 3/6\n",
      "799629/799629 [==============================] - 106s 132us/step - loss: 0.8340 - val_loss: 0.8454\n",
      "Epoch 4/6\n",
      "799629/799629 [==============================] - 102s 127us/step - loss: 0.8281 - val_loss: 0.8377\n",
      "Epoch 5/6\n",
      "799629/799629 [==============================] - 106s 132us/step - loss: 0.8246 - val_loss: 0.8351\n",
      "Epoch 6/6\n",
      "799629/799629 [==============================] - 123s 154us/step - loss: 0.8227 - val_loss: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c136860>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.fit([trn[:,0],trn[:,1]],trn[:,2],\n",
    "            validation_data=([val[:,0],val[:,1]],val[:,2]),\n",
    "            epochs=6, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear model\n",
    "```\n",
    "n_factor =10\n",
    "\n",
    "Train on 799442 samples, validate on 200767 samples\n",
    "Epoch 1/6\n",
    "799442/799442 [==============================] - 37s 46us/step - loss: 1.2824 - val_loss: 0.8614\n",
    "Epoch 2/6\n",
    "799442/799442 [==============================] - 36s 45us/step - loss: 0.8466 - val_loss: 0.8438\n",
    "Epoch 3/6\n",
    "799442/799442 [==============================] - 39s 48us/step - loss: 0.8343 - val_loss: 0.8383\n",
    "Epoch 4/6\n",
    "799442/799442 [==============================] - 35s 44us/step - loss: 0.8285 - val_loss: 0.8345\n",
    "Epoch 5/6\n",
    "799442/799442 [==============================] - 36s 45us/step - loss: 0.8250 - val_loss: 0.8329\n",
    "Epoch 6/6\n",
    "799442/799442 [==============================] - 36s 45us/step - loss: 0.8225 - val_loss: 0.8317\n",
    "```\n",
    "\n",
    "```\n",
    "n_factor=50\n",
    "\n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/6\n",
    "799629/799629 [==============================] - 102s 128us/step - loss: 1.1326 - val_loss: 0.8708\n",
    "Epoch 2/6\n",
    "799629/799629 [==============================] - 101s 126us/step - loss: 0.8484 - val_loss: 0.8476\n",
    "Epoch 3/6\n",
    "799629/799629 [==============================] - 106s 132us/step - loss: 0.8340 - val_loss: 0.8454\n",
    "Epoch 4/6\n",
    "799629/799629 [==============================] - 102s 127us/step - loss: 0.8281 - val_loss: 0.8377\n",
    "Epoch 5/6\n",
    "799629/799629 [==============================] - 106s 132us/step - loss: 0.8246 - val_loss: 0.8351\n",
    "Epoch 6/6\n",
    "799629/799629 [==============================] - 123s 154us/step - loss: 0.8227 - val_loss: 0.8327\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn model\n",
    "$$(u,m)\\to (e_u,e_m)\\to^{MLP} output$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn_model(n_factors=10):\n",
    "    in_u = Input(shape=(1,))\n",
    "    in_m = Input(shape=(1,))\n",
    "    \n",
    "    e_u = Embedding(input_dim=n_users, output_dim=n_factors)(in_u)\n",
    "    e_m = Embedding(input_dim=n_movies, output_dim=n_factors)(in_m)\n",
    "    \n",
    "    x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "    \n",
    "    x= Dense(units=int((2 * n_factors) *0.75), activation='relu')(x)\n",
    "    x= Dropout(0.4)(x)\n",
    "#    x= Dense(int((2*n_factors) *0.2), activation='relu')(x)    \n",
    "    \n",
    "    x = Dense(units=1)(x)\n",
    "    \n",
    "    return Model(inputs=[in_u,in_m], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_model = get_nn_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03734574],\n",
       "       [ 0.04256944],\n",
       "       [ 0.00699022],\n",
       "       [ 0.04564754],\n",
       "       [ 0.04943538],\n",
       "       [ 0.04825059],\n",
       "       [ 0.03127331],\n",
       "       [ 0.02851982],\n",
       "       [ 0.0427456 ],\n",
       "       [ 0.00544166]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.predict([trn[:10,0],trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800697 samples, validate on 199512 samples\n",
      "Epoch 1/6\n",
      "800697/800697 [==============================] - 121s 152us/step - loss: 1.0590 - val_loss: 0.8328\n",
      "Epoch 2/6\n",
      "800697/800697 [==============================] - 118s 147us/step - loss: 0.8299 - val_loss: 0.8071\n",
      "Epoch 3/6\n",
      "800697/800697 [==============================] - 127s 159us/step - loss: 0.8003 - val_loss: 0.7891\n",
      "Epoch 4/6\n",
      "800697/800697 [==============================] - 121s 151us/step - loss: 0.7843 - val_loss: 0.7827\n",
      "Epoch 5/6\n",
      "800697/800697 [==============================] - 121s 151us/step - loss: 0.7749 - val_loss: 0.7780\n",
      "Epoch 6/6\n",
      "800697/800697 [==============================] - 121s 151us/step - loss: 0.7682 - val_loss: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e41b048>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit([trn[:,0],trn[:,1]],trn[:,2],\n",
    "            validation_data=([val[:,0],val[:,1]],val[:,2]),\n",
    "            epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-layer model\n",
    "\n",
    "Model 1\n",
    "\n",
    "```\n",
    "n_factor =10\n",
    "\n",
    "x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "x= Dense(units=int((2 * n_factors) *3 /4), activation='relu')(x)\n",
    "x= Dropout(0.4)(x)\n",
    "x = Dense(units=1)(x)\n",
    "\n",
    "Train on 799442 samples, validate on 200767 samples\n",
    "Epoch 1/6\n",
    "799442/799442 [==============================] - 42s 53us/step - loss: 1.3456 - val_loss: 0.8327\n",
    "Epoch 2/6\n",
    "799442/799442 [==============================] - 48s 60us/step - loss: 0.8526 - val_loss: 0.8213\n",
    "Epoch 3/6\n",
    "799442/799442 [==============================] - 42s 53us/step - loss: 0.8411 - val_loss: 0.8149\n",
    "Epoch 4/6\n",
    "799442/799442 [==============================] - 39s 49us/step - loss: 0.8328 - val_loss: 0.8101\n",
    "Epoch 5/6\n",
    "799442/799442 [==============================] - 44s 56us/step - loss: 0.8285 - val_loss: 0.8083\n",
    "Epoch 6/6\n",
    "799442/799442 [==============================] - 42s 53us/step - loss: 0.8258 - val_loss: 0.8078\n",
    "```\n",
    "\n",
    "Model 2\n",
    "```\n",
    "n_factor=50\n",
    "\n",
    "Same architecture as model 1\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 121s 152us/step - loss: 1.0590 - val_loss: 0.8328\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 118s 147us/step - loss: 0.8299 - val_loss: 0.8071\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 127s 159us/step - loss: 0.8003 - val_loss: 0.7891\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 121s 151us/step - loss: 0.7843 - val_loss: 0.7827\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 121s 151us/step - loss: 0.7749 - val_loss: 0.7780\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 121s 151us/step - loss: 0.7682 - val_loss: 0.7754\n",
    "```\n",
    "\n",
    "Model 3\n",
    "```\n",
    "n_factor =10\n",
    "\n",
    "x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "x = Dropout(0.3)(x)\n",
    "x= Dense(units=int((2 * n_factors) *7 /10), activation='relu')(x)\n",
    "x = Dropout(0.75)(x)    \n",
    "x = Dense(units=1)(x)\n",
    "\n",
    "Train on 799442 samples, validate on 200767 samples\n",
    "Epoch 1/3\n",
    "799442/799442 [==============================] - 47s 59us/step - loss: 1.7085 - val_loss: 0.8877\n",
    "Epoch 2/3\n",
    "799442/799442 [==============================] - 45s 56us/step - loss: 0.9742 - val_loss: 0.8830\n",
    "Epoch 3/3\n",
    "799442/799442 [==============================] - 45s 56us/step - loss: 0.9638 - val_loss: 0.8638\n",
    "```\n",
    "\n",
    "Model 4\n",
    "```\n",
    "n_factor =50\n",
    "\n",
    "Same architecture as model 3\n",
    "\n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/3\n",
    "799629/799629 [==============================] - 114s 143us/step - loss: 1.2233 - val_loss: 0.8412\n",
    "Epoch 2/3\n",
    "799629/799629 [==============================] - 109s 136us/step - loss: 0.8714 - val_loss: 0.8244\n",
    "Epoch 3/3\n",
    "799629/799629 [==============================] - 113s 141us/step - loss: 0.8599 - val_loss: 0.8223\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-layer model\n",
    "\n",
    "Model 1\n",
    "```\n",
    "n_factor=10\n",
    "\n",
    "x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "x= Dense(units=int((2 * n_factors) *0.75), activation='relu')(x)\n",
    "x= Dropout(0.7)(x)\n",
    "x= Dense(int((2*n_factors) *0.3), activation='relu')(x)\n",
    "x= Dropout(0.7)(x)    \n",
    "x = Dense(units=1)(x)\n",
    "\n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 2.0640 - val_loss: 1.0601\n",
    "Epoch 2/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 1.1302 - val_loss: 1.0580\n",
    "Epoch 3/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 1.1178 - val_loss: 0.9833\n",
    "Epoch 4/6\n",
    "799629/799629 [==============================] - 40s 50us/step - loss: 1.0784 - val_loss: 0.9851\n",
    "Epoch 5/6\n",
    "799629/799629 [==============================] - 41s 51us/step - loss: 1.0762 - val_loss: 0.9842\n",
    "Epoch 6/6\n",
    "799629/799629 [==============================] - 38s 48us/step - loss: 1.0753 - val_loss: 0.9833\n",
    "```\n",
    "\n",
    "Model 2\n",
    "\n",
    "```\n",
    "n_factor=50\n",
    "\n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/3\n",
    "799629/799629 [==============================] - 116s 145us/step - loss: 1.3015 - val_loss: 0.8801\n",
    "Epoch 2/3\n",
    "799629/799629 [==============================] - 115s 144us/step - loss: 0.9299 - val_loss: 0.8609\n",
    "Epoch 3/3\n",
    "799629/799629 [==============================] - 113s 141us/step - loss: 0.9112 - val_loss: 0.8452\n",
    "```\n",
    "\n",
    "Model 3: as an extension of model 1 in two-layer models\n",
    "```\n",
    "n_factor=10\n",
    "\n",
    "    x = Flatten()(Concatenate()([e_u,e_m]))\n",
    "    x= Dense(units=int((2 * n_factors) *0.75), activation='relu')(x)\n",
    "    x= Dropout(0.4)(x)\n",
    "    x= Dense(int((2*n_factors) *0.2), activation='relu')(x)    \n",
    "    x = Dense(units=1)(x)\n",
    "    \n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 1.0733 - val_loss: 0.8548\n",
    "Epoch 2/6\n",
    "799629/799629 [==============================] - 38s 47us/step - loss: 0.8327 - val_loss: 0.8428\n",
    "Epoch 3/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 0.8184 - val_loss: 0.8414\n",
    "Epoch 4/6\n",
    "799629/799629 [==============================] - 39s 49us/step - loss: 0.8100 - val_loss: 0.8302\n",
    "Epoch 5/6\n",
    "799629/799629 [==============================] - 39s 48us/step - loss: 0.8035 - val_loss: 0.8424\n",
    "Epoch 6/6\n",
    "799629/799629 [==============================] - 37s 46us/step - loss: 0.7974 - val_loss: 0.8293\n",
    "```\n",
    "\n",
    "Model 5: adding batch norm\n",
    "```\n",
    "n_factor =10\n",
    "\n",
    "    x = Flatten()(Concatenate()([e_u,e_m]))    \n",
    "    x= Dense(units=int((2 * n_factors) *0.75), activation='relu')(x)\n",
    "    x= Dropout(0.4)(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Dense(int((2*n_factors) *0.2), activation='relu')(x)\n",
    "    x= BatchNormalization()(x)    \n",
    "    x = Dense(units=1)(x)\n",
    "    \n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/6\n",
    "799629/799629 [==============================] - 55s 69us/step - loss: 1.2542 - val_loss: 0.8618\n",
    "Epoch 2/6\n",
    "799629/799629 [==============================] - 52s 65us/step - loss: 0.8678 - val_loss: 0.8621\n",
    "Epoch 3/6\n",
    "799629/799629 [==============================] - 51s 64us/step - loss: 0.8469 - val_loss: 0.8409\n",
    "Epoch 4/6\n",
    "799629/799629 [==============================] - 53s 66us/step - loss: 0.8335 - val_loss: 0.8379\n",
    "Epoch 5/6\n",
    "799629/799629 [==============================] - 52s 65us/step - loss: 0.8278 - val_loss: 0.8402\n",
    "Epoch 6/6\n",
    "799629/799629 [==============================] - 52s 65us/step - loss: 0.8238 - val_loss: 0.8340\n",
    "```\n",
    "\n",
    "Model 6\n",
    "```\n",
    "n_factor =50\n",
    "\n",
    "Train on 799629 samples, validate on 200580 samples\n",
    "Epoch 1/6\n",
    "799629/799629 [==============================] - 137s 172us/step - loss: 1.0966 - val_loss: 0.8391\n",
    "Epoch 2/6\n",
    "799629/799629 [==============================] - 149s 186us/step - loss: 0.8313 - val_loss: 0.8144\n",
    "Epoch 3/6\n",
    "799629/799629 [==============================] - 141s 177us/step - loss: 0.8030 - val_loss: 0.7940\n",
    "Epoch 4/6\n",
    "799629/799629 [==============================] - 151s 189us/step - loss: 0.7845 - val_loss: 0.7869\n",
    "Epoch 5/6\n",
    "799629/799629 [==============================] - 161s 201us/step - loss: 0.7722 - val_loss: 0.7855\n",
    "Epoch 6/6\n",
    "799629/799629 [==============================] - 154s 193us/step - loss: 0.7637 - val_loss: 0.7901\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### todo: NN model with residual block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mixed model 1: MF+NN modeling of bias\n",
    "\n",
    "linear\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to e_u\\cdot e_m + Wb_u+W'b_m$$\n",
    "\n",
    "DNN\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to e_u\\cdot e_m + DNN(b_u,b_m)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mixed_model(e_factors = 10, b_factors = 10):\n",
    "    \n",
    "    u_input = Input(shape = (1,))\n",
    "    m_input = Input(shape = (1,))\n",
    "    \n",
    "    u_emb = Embedding(input_dim=n_users, output_dim=e_factors)(u_input)\n",
    "    m_emb = Embedding(input_dim=n_movies, output_dim=e_factors)(m_input)\n",
    "    dot = Flatten()(Dot(axes=2)([u_emb,m_emb]))\n",
    "    \n",
    "    bu = Embedding(input_dim=n_users, output_dim=b_factors)(u_input)\n",
    "    bm = Embedding(input_dim=n_movies, output_dim=b_factors)(m_input)\n",
    "#    bias = Dense(int((2*b_factors) *0.75), \n",
    "#                 kernel_regularizer=regularizers.l2(0.01),\n",
    "#                bias_regularizer=regularizers.l2(0.01))(Concatenate()([bu,bm]))\n",
    "    bias = Dense(int((2*b_factors) *0.75))(Concatenate()([bu,bm]))\n",
    "    bias = Dropout(0.4)(bias)\n",
    "    bias = Dense(1)(bias)\n",
    "    \n",
    "    out = Flatten()(Add()([dot,bias]))\n",
    "    out = Lambda(lambda x:5*x)(Activation('sigmoid')(out))\n",
    "\n",
    "    model = Model(inputs=[u_input,m_input], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.49259472],\n",
       "       [ 2.44761515],\n",
       "       [ 2.45221663],\n",
       "       [ 2.50785089],\n",
       "       [ 2.52457094],\n",
       "       [ 2.44535422],\n",
       "       [ 2.49942732],\n",
       "       [ 2.510396  ],\n",
       "       [ 2.44880247],\n",
       "       [ 2.46320534]], dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_model = get_mixed_model(e_factors=10, b_factors=50)\n",
    "mixed_model.compile(optimizer='adam', loss='mse')\n",
    "mixed_model.predict([trn[:10,0], trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800697 samples, validate on 199512 samples\n",
      "Epoch 1/6\n",
      "800697/800697 [==============================] - 136s 170us/step - loss: 0.8679 - val_loss: 0.8090\n",
      "Epoch 2/6\n",
      "800697/800697 [==============================] - 135s 169us/step - loss: 0.7589 - val_loss: 0.7651\n",
      "Epoch 3/6\n",
      "800697/800697 [==============================] - 153s 191us/step - loss: 0.7000 - val_loss: 0.7534\n",
      "Epoch 4/6\n",
      "800697/800697 [==============================] - 166s 207us/step - loss: 0.6675 - val_loss: 0.7536\n",
      "Epoch 5/6\n",
      "800697/800697 [==============================] - 158s 198us/step - loss: 0.6486 - val_loss: 0.7572\n",
      "Epoch 6/6\n",
      "800697/800697 [==============================] - 158s 198us/step - loss: 0.6363 - val_loss: 0.7639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f2864a8>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_model.fit([trn[:,0],trn[:,1]],trn[:,2],\n",
    "               validation_data=([val[:,0],val[:,1]],val[:,2]),\n",
    "               epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: linear\n",
    "```\n",
    "e_factor = b_factor =10\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 51s 64us/step - loss: 1.2682 - val_loss: 0.8255\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 49s 61us/step - loss: 0.7698 - val_loss: 0.7734\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 43s 54us/step - loss: 0.7064 - val_loss: 0.7596\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 43s 54us/step - loss: 0.6718 - val_loss: 0.7575\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 44s 56us/step - loss: 0.6516 - val_loss: 0.7581\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 44s 55us/step - loss: 0.6385 - val_loss: 0.7594\n",
    "```\n",
    "\n",
    "Model 2: linear + sigmoid output\n",
    "```\n",
    "e_factor = b_fac =10\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 52s 66us/step - loss: 0.8753 - val_loss: 0.8059\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 52s 64us/step - loss: 0.7571 - val_loss: 0.7720\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 53s 67us/step - loss: 0.7025 - val_loss: 0.7621\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 42s 53us/step - loss: 0.6698 - val_loss: 0.7616\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 46s 57us/step - loss: 0.6491 - val_loss: 0.7635\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 45s 56us/step - loss: 0.6353 - val_loss: 0.7645\n",
    "\n",
    "<keras.callbacks.History at 0x11d916e10>\n",
    "```\n",
    "\n",
    "Model 3: DNN + sigmoid\n",
    "```\n",
    "e_factor= b_fac =10\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 61s 77us/step - loss: 0.8847 - val_loss: 0.8075\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 58s 72us/step - loss: 0.7677 - val_loss: 0.7670\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 60s 75us/step - loss: 0.7085 - val_loss: 0.7566\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 61s 76us/step - loss: 0.6764 - val_loss: 0.7578\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 61s 76us/step - loss: 0.6566 - val_loss: 0.7610\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 64s 80us/step - loss: 0.6430 - val_loss: 0.7617\n",
    "\n",
    "<keras.callbacks.History at 0x11dcb6f28>\n",
    "```\n",
    "\n",
    "Model 4: DNN + sigmoid\n",
    "```\n",
    "e_factor=50,  b_fac =10\n",
    "\n",
    "Greatly overfitting even with regularzation!\n",
    "\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 138s 172us/step - loss: 0.8801 - val_loss: 0.7802\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 138s 172us/step - loss: 0.6779 - val_loss: 0.7570\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 140s 175us/step - loss: 0.5520 - val_loss: 0.7897\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 137s 171us/step - loss: 0.4784 - val_loss: 0.8364\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 138s 172us/step - loss: 0.4349 - val_loss: 0.8783\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 143s 178us/step - loss: 0.4059 - val_loss: 0.9133\n",
    "\n",
    "<keras.callbacks.History at 0x12108fc88>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed model 2: MF + NN modeling of interaction\n",
    "\n",
    "linear model\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to We_u\\cdot W'e_m+b_u+b_m$$\n",
    "\n",
    "DNN\n",
    "$$(u,m)\\to (e_u,e_m,b_u,b_m)\\to DNN(e_u)\\cdot DNN'(e_m)+b_u+b_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mixed_nn_interaction(n_factors = 10):\n",
    "    \n",
    "    u_input = Input(shape = (1,))\n",
    "    u_emb = Embedding(input_dim=n_users, output_dim=n_factors)(u_input)\n",
    "    m_input = Input(shape = (1,))\n",
    "    m_emb = Embedding(input_dim=n_movies, output_dim=n_factors)(m_input)\n",
    "    \n",
    "    out_u= Dense(int(n_factors *0.75))(u_emb)\n",
    "    out_m= Dense(int(n_factors *0.75))(m_emb)\n",
    "    dot = Flatten()(Dot(axes=2)([out_u,out_m]))\n",
    "    \n",
    "    u_b = Embedding(input_dim=n_users, output_dim=1)(u_input)\n",
    "    m_b = Embedding(input_dim=n_movies, output_dim=1)(m_input)\n",
    "    \n",
    "    out = Flatten()(Add()([dot,u_b,m_b]))\n",
    "    \n",
    "    out = Activation('sigmoid')(out)\n",
    "    out = Lambda(lambda x:5*x)(out)\n",
    "\n",
    "    model = Model(inputs=[u_input,m_input], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.40735555],\n",
       "       [ 2.48720074],\n",
       "       [ 2.4142499 ],\n",
       "       [ 2.43169713],\n",
       "       [ 2.45257306],\n",
       "       [ 2.45953631],\n",
       "       [ 2.45998096],\n",
       "       [ 2.46519303],\n",
       "       [ 2.41099477],\n",
       "       [ 2.50233197]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_interaction_model = get_mixed_nn_interaction(n_factors=50)\n",
    "nn_interaction_model.compile(optimizer='adam', loss='mse')\n",
    "nn_interaction_model.predict([trn[:10,0], trn[:10,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800697 samples, validate on 199512 samples\n",
      "Epoch 1/6\n",
      "800697/800697 [==============================] - 130s 163us/step - loss: 0.8498 - val_loss: 0.7934\n",
      "Epoch 2/6\n",
      "800697/800697 [==============================] - 125s 156us/step - loss: 0.7452 - val_loss: 0.7635\n",
      "Epoch 3/6\n",
      "800697/800697 [==============================] - 126s 157us/step - loss: 0.6724 - val_loss: 0.7634\n",
      "Epoch 4/6\n",
      "800697/800697 [==============================] - 131s 164us/step - loss: 0.6152 - val_loss: 0.7709\n",
      "Epoch 5/6\n",
      "800697/800697 [==============================] - 123s 154us/step - loss: 0.5718 - val_loss: 0.7900\n",
      "Epoch 6/6\n",
      "800697/800697 [==============================] - 118s 147us/step - loss: 0.5391 - val_loss: 0.8056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12206dba8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_interaction_model.fit([trn[:,0],trn[:,1]],trn[:,2],\n",
    "               validation_data=([val[:,0],val[:,1]],val[:,2]),\n",
    "               epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: linear, n_factor =10\n",
    "```\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 49s 62us/step - loss: 0.8666 - val_loss: 0.8097\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 42s 53us/step - loss: 0.7823 - val_loss: 0.7807\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 43s 54us/step - loss: 0.7541 - val_loss: 0.7710\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 47s 59us/step - loss: 0.7328 - val_loss: 0.7626\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 45s 56us/step - loss: 0.7113 - val_loss: 0.7565\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 46s 57us/step - loss: 0.6962 - val_loss: 0.7502\n",
    "\n",
    "<keras.callbacks.History at 0x120b8e080>\n",
    "```\n",
    "\n",
    "Model 2: linear, n_factor =50, overfitting\n",
    "```\n",
    "Train on 800697 samples, validate on 199512 samples\n",
    "Epoch 1/6\n",
    "800697/800697 [==============================] - 130s 163us/step - loss: 0.8498 - val_loss: 0.7934\n",
    "Epoch 2/6\n",
    "800697/800697 [==============================] - 125s 156us/step - loss: 0.7452 - val_loss: 0.7635\n",
    "Epoch 3/6\n",
    "800697/800697 [==============================] - 126s 157us/step - loss: 0.6724 - val_loss: 0.7634\n",
    "Epoch 4/6\n",
    "800697/800697 [==============================] - 131s 164us/step - loss: 0.6152 - val_loss: 0.7709\n",
    "Epoch 5/6\n",
    "800697/800697 [==============================] - 123s 154us/step - loss: 0.5718 - val_loss: 0.7900\n",
    "Epoch 6/6\n",
    "800697/800697 [==============================] - 118s 147us/step - loss: 0.5391 - val_loss: 0.8056\n",
    "\n",
    "<keras.callbacks.History at 0x12206dba8>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
